\documentclass[10pt]{beamer}

\usetheme[background=dark, numbering=fraction]{metropolis}
\usepackage{appendixnumberbeamer}
\usepackage{color, xcolor}
\usepackage{dsfont}
\definecolor{mydarkblue}{rgb}{0,0.08,0.45}
\usepackage{wrapfig}

\usepackage{booktabs}
\usepackage[scale=2]{ccicons}


\usepackage{adjustbox} 
\usepackage{pgfplots}
\usepgfplotslibrary{dateplot}
\usepackage{fourier-orns}
\usepackage{xspace}

\usepackage[scale=2]{ccicons}
\usepackage{fontawesome}


\usepackage[backend=bibtex,style=authoryear]{biblatex}
\addbibresource{index.bib}
\renewcommand*{\bibfont}{\scriptsize}
\let\oldcite=\cite                                                              
\renewcommand{\cite}[1]{\textcolor[rgb]{.7,.7,.7}{\oldcite{#1}}}
\let\oldparencite=\parencite                                                         
\renewcommand{\parencite}[1]{\textcolor[rgb]{.7,.7,.7}{\oldparencite{#1}}}


\newcommand{\themename}{\textbf{\textsc{metropolis}}\xspace}
% \DeclareMathOperator*{\argmin}{arg\,min}
% \DeclareMathOperator*{\minimize}{minimize}
% \DeclareMathOperator*{\maximize}{maximize}

\makeatletter
\renewcommand{\metropolis@colors@dark}{%
  \setbeamercolor{normal text}{%
    fg=black!2,
    bg=mDarkTeal
  }%
  \usebeamercolor[fg]{normal text}%
}
\renewcommand{\metropolis@colors@light}{%
  \setbeamercolor{normal text}{%
    fg=mDarkTeal,
    bg=black!2
  }%
  \usebeamercolor[fg]{normal text}%
}
\makeatother

\usepackage{tikz}
\usetikzlibrary{matrix,chains,positioning,decorations.pathreplacing,arrows}


\title{\huge Parallel Optimization and Machine Learning}
\author{
\large{\bfseries Fabian Pedregosa}}
% {\normalsize{ includes work with Remi Leblond and Simon Lacoste-Julien}}}
% \author{
% \hspace{4.8em}\includegraphics[width=0.2\linewidth]{img/remi}
% \hspace{4.8em}\includegraphics[width=0.2\linewidth]{img/SLJ}
% }\\
% {\normalsize\vphantom{}\hspace{0.5em} Fabian Pedregosa \hspace{3.5em} R\'emi Leblond \hspace{2.2em} Simon Lacoste--Julien}\\
% \vphantom{}\\
% \vphantom{}\includegraphics[width=0.3\linewidth]{img/logo_inria}
% \includegraphics[width=0.12\linewidth]{img/ens.png}
% \hspace{0.5em}\includegraphics[width=0.3\linewidth]{img/montreal}
% \hspace{0.5em}\includegraphics[width=0.2\linewidth]{img/mila}
% }
% 
\institute{

{\centerline{\hspace{2em}\includegraphics[width=0.3\linewidth]{img/Berkeley_wordmark_gold_no_uc}\hspace*{2em}
\includegraphics[width=0.3\linewidth]{img/eth_logo_kurz_neg}\hspace*{2em}
{\includegraphics[width=0.25\linewidth]{img/eu}}
}}
}
%\titlegraphic{\hfill\includegraphics[height=1.5cm]{figures/UCBerkeley_wordmark_blue.eps}}
\date{\vspace{1em}\today~{Huawei Paris Research Center}}

\input defs.tex



\begin{document}

\maketitle

\metroset{background=light} % change background theme according to manual
{
\usebackgroundtemplate{%
\begin{picture}(40,260)
  \includegraphics[height=0.9\paperheight]{img/white-rabbit}
  \end{picture}
  }
\begin{frame}{About me}
\begin{columns}
\begin{column}{0.4\textwidth}  %%<--- here
\end{column}
\begin{column}{0.6\textwidth}  %%<--- here
\begin{itemize}
\item Engineer (2010-2012), Inria Saclay (scikit-learn kickstart).
\item PhD (2012-2015, Inria Saclay)
\item Postdoc (2015-2016), Dauphine--ENS--Inria Paris.
\item Postdoc (2017-present), UC Berkeley - ETH Zurich (Marie-Curie fellowship, European Commission)
\end{itemize}
\end{column}
\end{columns}

\note{I should start by presenting myself. I started my career as an engineer }

\end{frame}
}

\metroset{background=dark} % change background theme according to manual

\begin{frame}{Motivation}
\begin{columns}[T] % align columns
\begin{column}{.5\textwidth}
\centering{Computer add in 1993}
\includegraphics[width=1.05\linewidth]{img/old_ad}
\end{column}%
\hfill%
\begin{column}{.5\textwidth}
\centering{Computer add in 2006}
\includegraphics[width=\linewidth]{img/2006_ad}
\end{column}%
\end{columns}
What has changed?
\pause
2006 = no longer mentions to speed of processors
\end{frame}


\begin{frame}{Moore's law}
\begin{quote}
The complexity for minimum component costs 
has increased at a rate of roughly a factor of 
two per year. Certainly over the short term this 
rate can be expected to continue
\end{quote}
Gordon Moore (Intel), 1965

\hspace{1em}\begin{quote}
OK, maybe a factor of two every two 
years.
\end{quote}
Gordon Moore (Intel), 1975 [paraphrased]
\end{frame}


\begin{frame}{40 years of CPU trends}
\centering\includegraphics[width=0.8\linewidth]{img/moore_law}

\begin{itemize}[<+->]
\item Speed of CPUs has stagnated since 2005;
\item Multi-core architectures are here to stay.
\end{itemize}
\pause {\bfseries Parallel algorithms needed to take advantage of modern CPUs.}
\end{frame}


% \begin{frame}{Outline}
% 
% {\bfseries Goal of the talk:} overview and state of the art in parallel optimization methods in machine learning.
% \end{frame}




\begin{frame}{Parallel optimization}
Parallel algorithms can be divided  into two large categories: 
{\bfseries synchronous} and {\bfseries asynchronous}.

\begin{figure}
\vspace{-1em}\begin{flushright}\vspace{-1em}{\scriptsize Image credits: \parencite{peng2016arock}}\end{flushright}
\vspace{-0.5em}\includegraphics[width=0.8\linewidth]{img/sync_vs_async}
\vspace{-0.5em}\end{figure}


\begin{columns}[T] % align columns
\begin{column}{.5\textwidth}
{\centering \bfseries Synchronous methods}

\vspace{0.5em}
~\faCheck~Easy to implement (i.e., developed software packages).

\vspace{0.5em}
~\faCheck~Well understood.

\vspace{0.5em}
~\faClose~Limited speedup due to synchronization costs..
\end{column}
\begin{column}{.5\textwidth}
{\centering \bfseries Asynchronous methods}

\vspace{0.5em}
~\faCheck~Faster, typically larger speedups.

\vspace{0.5em}
~\faClose~Not well understood, large gap between theory and practice.

\vspace{0.5em}
~\faClose~No mature software solutions.

\end{column}%
\end{columns}

\end{frame}



\begin{frame}{Outline}

{\centering \bfseries Synchronous methods}
\begin{itemize}
\item Synchronous (stochastic) gradient descent.
\end{itemize}
{\centering \bfseries Asynchronous methods}
\begin{itemize}
\item Asynchronous SGD (Hogwild)~\parencite{hogwild2011}
\item Asynchronous variance-reduced stochastic methods~\parencite{leblond2016Asaga}, \parencite{pedregosa2017proxasaga}.
\item Analysis of asynchronous methods. 
\item Codes and implementation aspects.
\end{itemize}

Leaving out many parallel synchronous methods: ADMM~\parencite{glowinski1975approximation}, CoCoA~\parencite{NIPS2014_5599}, DANE~\parencite{shamir2014communication}, to name a few.

\end{frame}

\begin{frame}{Outline}
Most of this is joint work with Remi Leblond and Simon Lacoste-Julien
\end{frame}


\section{Synchronous algorithms}

\begin{frame}{Optimization for machine learning}
Large part of problems in machine learning can be framed as optimization problems of the form
$$\minimize_x f(x) \defas \displaystyle\frac{1}{n}\sum^{n}_{i=1} f_i(x)$$
\begin{columns}
\begin{column}{0.6\textwidth}  %%<--- here
{\bfseries Gradient descent} \parencite{cauchy1847methode}. Descend along steepest direction ($-\nabla f(x)$)
$$x^+ = x - \gamma \nabla f(x)$$

{\bfseries Stochastic gradient descent} (SGD) \parencite{robbins1951sgd}. Select a random index $i$ and descent along $~\nabla f_i(x)$:
$$x^+ = x - \gamma \nabla f_i(x)$$

\end{column}
\begin{column}{0.4\textwidth}  %%<--- here
\includegraphics[width=\linewidth]{img/gd}

\vspace{2em}
\includegraphics[width=\linewidth]{img/sgd}
\vspace*{-2.5em}\begin{flushright}{\footnotesize images source: Francis Bach}\end{flushright}
\end{column}
\end{columns}
\end{frame}






\begin{frame}{Parallelizing gradient descent}

Bottleneck in gradient descent $=$ compute $\nabla f(\xx)$.

{\bfseries Idea:} Parallelize computation of $\nabla f(\xx)$:
\begin{align*}
\nabla f(\xx) &= \frac{1}{n} \sum_{i=1} \nabla f_i(\xx)\\ \pause
&= X
\end{align*}
\begin{center}
{\centering\includegraphics[width=0.5\linewidth]{img/mapreduce}}
\end{center}
\end{frame}

\begin{frame}{Parallel SGD}
This can also be extended to stochastic gradient descent.

Instead of choosing one sample, choose a subset of $k$ samples, and compute their gradient in parallel.

\begin{wrapfigure}{r}{0.3\textwidth} 
\includegraphics[width=\linewidth]{img/gpu}
\end{wrapfigure}
This is the parallelization implemented in deep learning software: Tensorflow, PyTorch, etc.
\end{frame}

\section{Asynchronous algorithms}

\begin{frame}{Asynchronous SGD}

\begin{columns}
\begin{column}{0.5\textwidth}  %%<--- here
Synchronization is the bottleneck. \\

\vspace{1em}{\bfseries Idea:} What if we just ignore it?
\end{column}
\begin{column}{0.5\textwidth}  %%<--- here
\begin{center}
\includegraphics[width=\linewidth]{img/synchronization}
\end{center}
\end{column}
\end{columns}

Hogwild~\parencite{hogwild2011}: each core runs SGD in parallel, without synchronization, and updates the same vector of coefficients.

{\bfseries In theory}: convergence under very strong assumptions.

{\bfseries In practice}: just works.

\end{frame}

\begin{frame}{Hogwild in more detail}


Each core follows the same procedure
\begin{enumerate}
\item Read the information from shared memory $\hat{\xx}$
\item Sample $i \in \{1, \ldots, n\}$
\item Compute partial gradient $\nabla f_i(\hat{\xx})$
\item Write the SGD update to shared memory $\xx = \xx - \gamma \nabla f_i(\hat{\xx})$
\end{enumerate}
\pause
\includegraphics[width=0.6\linewidth]{img/async_updates}

\end{frame}


\begin{frame}{Hogwild}

Very popular method.



Hard to analyze

Large gap between theory and practice.
\end{frame}

\begin{frame}{Analysis of asynchronous methods}

Things become unpredictable in presence of asynchrony.


{\bfseries How to name the iterates?}

\begin{figure}
\vspace{-0.5em}\includegraphics[width=0.8\linewidth]{img/sync_vs_async}
\vspace{-0.5em}\end{figure}

\note{

Many of the things we are used to in classical optimization just break down in the presence of asynchrony. Suddenly, we need to start worrying for things that we used to give for granted. One these things is how to name things. It sounds silly but its true,  

}
\end{frame}


\begin{frame}{Naming scheme in Hogiwild}

{\bfseries Simple, intuitive and wrong}

the $t$-th iterate is the $t$-th time that any core has finished writing his update to shared memory. 

Existing analysis of Hogwild is wrong

Relies on unbiased estimate of the gradient


To break dependency we 
\end{frame}

\begin{frame}{XX}
SAGA, available in scikit-learn as
\end{frame}


\begin{frame}{Composite objective}

\faExclamationTriangle~These methods assume objective function is smooth

Cannot be applied to Lasso, Group Lasso, box constraints, etc.

\pause 
\vspace{5mm}
{\bfseries Objective}: minimize composite objective function:
$$
\minimize_{\xx} f(\xx) + h(\xx)~,~ \text{ with $f(x) = \textstyle \frac{1}{n}\sum_{i=1}^n f_i(\xx)$}
$$
where $f_i$ is smooth and $h$ is a block-separable (i.e., $h(\boldsymbol{x}) = \sum_{B} h([\boldsymbol{x}]_B)$) convex function for which we have access to its proximal operator.
\end{frame}



% \begin{frame}{Sparse Proximal SAGA}
% 
% {\bfseries Contribution 1: Sparse Proximal SAGA.} Variant of SAGA \parencite{defazio2014saga}, particularly efficient when $\nabla f_i$ are sparse.
% % \vspace{0.5em}\begin{equation*}\tag{SAGA}
% % \xx^+ = \prox_{\gamma h}(\xx - \gamma (\nabla f_i(\xx) - \balpha_i + \overline{\balpha}))\,;~\balpha_i^+ = \nabla f_i(\xx)~.
% % \end{equation*}
% 
% % \begin{align}
% %     \vphantom{\sum}\vv_i &= \nabla f_i(\xx) - \balpha_i + \tikzmarkin<2>{a}\DD_i\tikzmarkend{a} \overline{\balpha}\,\\
% %     \vphantom{\sum}\xx^+ &= \prox_{\gamma \varphi_i}\big(\xx - \gamma \vv_i \big)\,;~
% %     \balpha_i^+ = \nabla f_i(\xx)
% % \end{align}
% 
% \onslide<2->{Like SAGA, it relies on \rnode{a}{unbiased gradient} estimate} \onslide<3->{ and \rnode{b}{proximal step}}
% 
% % \[
% % \onslide<2->{
% %       \vv_i \rnode[t]{ae}{=} \nabla f_i(\xx) - \balpha_i + \DD_i \overline{\balpha}\,; }
% % \onslide<3->{%
% %   ~\xx^+ = \rnode[t]{be}{\prox}_{\gamma \varphi_i}(\xx - \gamma \vv_i)\,;~\balpha_i^+ = \nabla f_i(\xx)}
% % \] 
% %  \onslide<2->{\nccurve[angleA=-90,angleB=90, linecolor=gray]{->}{a}{ae}}
% %  \onslide<3->{\nccurve[angleA=-90, angleB=90, linecolor=gray]{->}{b}{be}}
%  
%  \onslide<4->{
%  {Unlike SAGA}, and $\varphi_i$ are designed to give sparse updates while verifying unbiasedness conditions.}
%  
% \onslide<5->{ {\bfseries Convergence:} same linear convergence rate as SAGA, with cheaper updates in presence of sparsity. 
%  }
% \end{frame}


\begin{frame}{Proximal Asynchronous SAGA (ProxASAGA)}

{\bfseries Contribution 2: Proximal Asynchronous SAGA (ProxASAGA).} Each core
 runs Sparse Proximal SAGA asynchronously without locks and updates $\xx$, $\balpha$ and $\overline{\balpha}$ in shared memory.

\vspace{0.5em} \faRandom~All read/write operations to shared memory are \emph{inconsistent}, i.e., no performance destroying vector-level locks while reading/writing.

{\bfseries Convergence:} under sparsity assumptions, ProxASAGA converges with the same rate as the sequential algorithm $\implies$ theoretical linear speedup with respect to the number of cores.
\end{frame}


\begin{frame}{Empirical results}



% \begin{columns}[T] % align columns
% \begin{column}{.35\textwidth}
% \end{column}%
% \hfill%
% \begin{column}{.65\textwidth}
% \includegraphics[width=1\linewidth]{img/prox_asaga_1}
% \end{column}%
% \end{columns}

ProxASAGA vs competing methods on 3 large-scale datasets, $\ell_1$-regularized logistic regression

\vspace{0em}\begingroup
\fontsize{9pt}{10pt}\selectfont
\begin{table}
\centering
%\resizebox{\linewidth}{!}{
\begin{tabular}{lrrrrr}
\toprule
{\bfseries\sffamily Dataset} & \multicolumn{1}{c}{$n$} & \multicolumn{1}{c}{$p$} & {{density}} & \multicolumn{1}{c}{$L$} & $\Delta$\\
\midrule
{\bfseries\sffamily KDD 2010} & \hfill 19,264,097 & \hfill 1,163,024 & \hfill $10^{-6}$ & \hfill 28.12 & 0.15\\
{\bfseries\sffamily KDD 2012} & \hfill 149,639,105 & \hfill 54,686,452 & \hfill $2 \times 10^{-7}$ & \hfill $1.25$ & 0.85\\
{\bfseries\sffamily Criteo} & \hfill 45,840,617 & \hfill 1,000,000 & \hfill $4 \times 10^{-5}$ & \hfill $1.25$ & 0.89\\
\bottomrule
\end{tabular}
%}
%\vspace{-5mm}
\end{table}
\endgroup


\includegraphics[width=1\linewidth]{img/prox_asaga_1}

\end{frame}


\begin{frame}{Empirical results - Speedup}

$$
\text{Speedup} = \frac{\text{Time to $10^{-10}$ suboptimality 
on one core}}{\text{Time to same suboptimality on $k$ cores}}
$$

\includegraphics[width=\linewidth]{img/prox_asaga_2}

\begin{itemize}
\pause \item ProxASAGA achieves speedups between 6x and 12x on a 20 cores architecture.

\pause \vspace{0.5em}\item As predicted by theory, there is a high correlation between degree of sparsity and speedup.
\end{itemize}

\pause Thanks for your attention, see you at {\bfseries poster \#159}.
\end{frame}

\begin{frame}{XX}

\faGithub~Code is in github: \url{https://github.com/fabianp/ProxASAGA}

A very efficient implementation of SAGA can be found in scikit-learn
\end{frame}



\begin{frame}{Supervised Machine Learning}

{\bfseries Data}: $n$ observations $(\boldsymbol{a}_i, b_i) \in \mathbb{R}^p \times \mathbb{R}$

{\bfseries Prediction function}: $h(\boldsymbol{a}, \boldsymbol{x}) \in \RR$


{\bfseries Motivating examples:}
\begin{itemize}
\item Linear prediction: $h(\boldsymbol{a}, \xx) = \xx^T\boldsymbol{a}$
\item Neural networks: $h(\boldsymbol{a}, \xx) = \xx_m^T \sigma(\xx_{m-1}\sigma(\cdots\xx_2^T\sigma(\xx_1^T\boldsymbol{a}))$
\end{itemize}

\only<1>{
\begin{tikzpicture}[
ampersand replacement=\&,
scale=0.6,
every node/.style={scale=0.6},
plain/.style={
  draw=none,
  fill=none,
  },
net/.style={
  matrix of nodes,
  nodes={
    draw,
    circle,
    inner sep=10pt
    },
  nodes in empty cells,
  column sep=2cm,
  row sep=-9pt
  },
>=latex
]
\matrix[net] (mat)
{
|[plain]| \parbox{1.3cm}{\centering \Large Input\\layer} \& |[plain]| \parbox{1.0cm}{\centering\Large Hidden\\layer} \& |[plain]| \parbox{1.3cm}{\centering \Large Output\\layer} \\
\& |[plain]| \\
|[plain]| \& \\
\& |[plain]| \\
  |[plain]| \& |[plain]| \\
\& \& \\
  |[plain]| \& |[plain]| \\
\& |[plain]| \\
  |[plain]| \& \\
\& |[plain]| \\    };
\foreach \ai [count=\mi ]in {2,4,...,10}
  \draw[<-] (mat-\ai-1) -- node[above] {$\boldsymbol{a}_{\mi}$} +(-2cm,0);
\foreach \ai in {2,4,...,10}
{\foreach \aii in {3,6,9}
  \draw[->] (mat-\ai-1) -- (mat-\aii-2);
}
\foreach \ai in {3,6,9}
  \draw[->] (mat-\ai-2) -- (mat-6-3);
\draw[->] (mat-6-3) -- node[above] {Ouput} +(2cm,0);
\end{tikzpicture}
}
\only<2>{
{\bfseries Minimize some distance (e.g., quadratic) between the prediction}
$$
\minimize_{\xx} \frac{1}{n}\sum_{i=1}^n \ell(\boldsymbol{b}_i, h(\boldsymbol{a}_i, \xx)) \stackrel{\text{notation}}{=} \frac{1}{n}\sum_{i=1}^n f_i(\xx)
$$
where popular examples of $\ell$ are
\begin{itemize}
\item Squared loss, $ \ell(\boldsymbol{b}_i, h(\boldsymbol{a}_i, \xx)) \defas (\boldsymbol{b}_i - h(\boldsymbol{a}_i, \xx))^2$
\item Logistic (softmax), $ \ell(\boldsymbol{b}_i, h(\boldsymbol{a}_i, \xx)) \defas \log(1 + \exp(-\boldsymbol{b}_i h(\boldsymbol{a}_i, \xx)))$
\end{itemize}
}

\note{
The kind of parallelization will depend on the problem.
The kind of problem that we will be interested in
The problem: supervised learning.
}


\end{frame}




\begin{frame}[noframenumbering, allowframebreaks]
\nocite{pedregosa2017proxasaga}
  % \frametitle{References}
  \printbibliography
 \end{frame}

\end{document}
