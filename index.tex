\documentclass[10pt]{beamer}

\usetheme[background=dark, numbering=fraction]{metropolis}
\usepackage{appendixnumberbeamer}
\usepackage{color, xcolor}
\usepackage{dsfont}
\definecolor{mydarkblue}{rgb}{0,0.08,0.45}
\usepackage{wrapfig}

\usepackage{booktabs}
\usepackage[scale=2]{ccicons}
\usepackage[beamer]{hf-tikz}


\usepackage{adjustbox} 
\usepackage{pgfplots}
\usepgfplotslibrary{dateplot}
\usepackage{fourier-orns}
\usepackage{xspace}
\usepackage{pst-node}
\newrgbcolor{lila}{0.6 0.2 0.5}
\newrgbcolor{darkyellow}{1 0.9 0}


% For algorithms
\usepackage{algorithm}
\usepackage{algorithmic}


\usepackage[scale=2]{ccicons}
\usepackage{fontawesome}


\usepackage[backend=bibtex,style=authoryear]{biblatex}
\addbibresource{index.bib}
\renewcommand*{\bibfont}{\scriptsize}
\let\oldcite=\cite                                                              
\renewcommand{\cite}[1]{\textcolor[rgb]{.7,.7,.7}{\oldcite{#1}}}
\let\oldparencite=\parencite                                                         
\renewcommand{\parencite}[1]{\textcolor[rgb]{1.,.9,.9}{\oldparencite{#1}}}


\newcommand{\themename}{\textbf{\textsc{metropolis}}\xspace}
% \DeclareMathOperator*{\argmin}{arg\,min}
% \DeclareMathOperator*{\minimize}{minimize}
% \DeclareMathOperator*{\maximize}{maximize}

\makeatletter
\renewcommand{\metropolis@colors@dark}{%
  \setbeamercolor{normal text}{%
    fg=black!2,
    bg=mDarkTeal
  }%
  \usebeamercolor[fg]{normal text}%
}
\renewcommand{\metropolis@colors@light}{%
  \setbeamercolor{normal text}{%
    fg=mDarkTeal,
    bg=black!2
  }%
  \usebeamercolor[fg]{normal text}%
}
\makeatother

\usepackage{tikz}
\usetikzlibrary{matrix,chains,positioning,decorations.pathreplacing,arrows}


\title{\huge Parallel Optimization in Machine Learning}
\author{
\large{\bfseries Fabian Pedregosa}}
% {\normalsize{ includes work with Remi Leblond and Simon Lacoste-Julien}}}
% \author{
% \hspace{4.8em}\includegraphics[width=0.2\linewidth]{img/remi}
% \hspace{4.8em}\includegraphics[width=0.2\linewidth]{img/SLJ}
% }\\
% {\normalsize\vphantom{}\hspace{0.5em} Fabian Pedregosa \hspace{3.5em} R\'emi Leblond \hspace{2.2em} Simon Lacoste--Julien}\\
% \vphantom{}\\
% \vphantom{}\includegraphics[width=0.3\linewidth]{img/logo_inria}
% \includegraphics[width=0.12\linewidth]{img/ens.png}
% \hspace{0.5em}\includegraphics[width=0.3\linewidth]{img/montreal}
% \hspace{0.5em}\includegraphics[width=0.2\linewidth]{img/mila}
% }
% 
\institute{

{\centerline{\hspace{2em}\includegraphics[width=0.3\linewidth]{img/Berkeley_wordmark_gold_no_uc}\hspace*{2em}
\includegraphics[width=0.3\linewidth]{img/eth_logo_kurz_neg}\hspace*{2em}
{\includegraphics[width=0.25\linewidth]{img/eu}}
}}
}
%\titlegraphic{\hfill\includegraphics[height=1.5cm]{figures/UCBerkeley_wordmark_blue.eps}}
\date{\vspace{1em}\today~{Huawei Paris Research Center}}

\input defs.tex



\begin{document}

\maketitle

\metroset{background=light} % change background theme according to manual
{
\usebackgroundtemplate{%
\begin{picture}(40,260)
  \includegraphics[height=0.9\paperheight]{img/white-rabbit}
  \end{picture}
  }
\begin{frame}{About me}
\begin{columns}
\begin{column}{0.4\textwidth}  %%<--- here
\end{column}
\begin{column}{0.6\textwidth}  %%<--- here
\begin{itemize}
\item Engineer (2010-2012), Inria Saclay (scikit-learn kickstart).
\item PhD (2012-2015, Inria Saclay)
\item Postdoc (2015-2016), Dauphine--ENS--Inria Paris.
\item Postdoc (2017-present), UC Berkeley - ETH Zurich (Marie-Curie fellowship, European Commission)
\end{itemize}
\end{column}
\end{columns}

\note{I should start by presenting myself. I started my career as an engineer }

\end{frame}
}

\metroset{background=dark} % change background theme according to manual

\begin{frame}{Motivation}
\begin{columns}[T] % align columns
\begin{column}{.5\textwidth}
\centering{Computer add in 1993}
\includegraphics[width=1.05\linewidth]{img/old_ad}
\end{column}%
\hfill%
\begin{column}{.5\textwidth}
\centering{Computer add in 2006}
\includegraphics[width=\linewidth]{img/2006_ad}
\end{column}%
\end{columns}
What has changed?
\pause

2006 = no longer mentions to speed of processors.

\pause
Primary feature: number of cores.
\end{frame}


\begin{frame}{Moore's law}
\begin{quote}
The complexity for minimum component costs 
has increased at a rate of roughly a factor of 
two per year. Certainly over the short term this 
rate can be expected to continue
\end{quote}
Gordon Moore (Intel), 1965

\hspace{1em}\begin{quote}
OK, maybe a factor of two every two 
years.
\end{quote}
Gordon Moore (Intel), 1975 [paraphrased]
\end{frame}


\begin{frame}{40 years of CPU trends}
\vspace{1em}
\centering\includegraphics[width=0.8\linewidth]{img/moore_law}

\begin{itemize}[<+->]
\item Speed of CPUs has stagnated since 2005.
\item Multi-core architectures are here to stay.
\end{itemize}
\pause {\bfseries Parallel algorithms needed to take advantage of modern CPUs.}
\end{frame}


% \begin{frame}{Outline}
% 
% {\bfseries Goal of the talk:} overview and state of the art in parallel optimization methods in machine learning.
% \end{frame}




\begin{frame}{Parallel optimization}
Parallel algorithms can be divided  into two large categories: 
{\bfseries synchronous} and {\bfseries asynchronous}.

\begin{figure}
\vspace{-1em}\begin{flushright}\vspace{-1em}{\scriptsize Image credits: \parencite{peng2016arock}}\end{flushright}
\vspace{-0.5em}\includegraphics[width=0.8\linewidth]{img/sync_vs_async}
\vspace{-0.5em}\end{figure}


\begin{columns}[T] % align columns
\begin{column}{.5\textwidth}
{\centering \bfseries Synchronous methods}

\vspace{0.5em}
~\faCheck~Easy to implement (i.e., developed software packages).

\vspace{0.5em}
~\faCheck~Well understood.

\vspace{0.5em}
~\faClose~Limited speedup due to synchronization costs..
\end{column}
\begin{column}{.5\textwidth}
{\centering \bfseries Asynchronous methods}

\vspace{0.5em}
~\faCheck~Faster, typically larger speedups.

\vspace{0.5em}
~\faClose~Not well understood, large gap between theory and practice.

\vspace{0.5em}
~\faClose~No mature software solutions.

\end{column}%
\end{columns}

\end{frame}



\begin{frame}{Outline}

{\centering \bfseries Synchronous methods}
\begin{itemize}
\item Synchronous (stochastic) gradient descent.
\end{itemize}
{\centering \bfseries Asynchronous methods}
\begin{itemize}
\item Asynchronous SGD (Hogwild)~\parencite{hogwild2011}
\item Asynchronous variance-reduced stochastic methods~\parencite{leblond2016Asaga}, \parencite{pedregosa2017proxasaga}.
\item Analysis of asynchronous methods. 
\item Codes and implementation aspects.
\end{itemize}

Leaving out many parallel synchronous methods: ADMM~\parencite{glowinski1975approximation}, CoCoA~\parencite{NIPS2014_5599}, DANE~\parencite{shamir2014communication}, to name a few.

\end{frame}

\begin{frame}{Outline}
Most of the following is joint work with RÃ©mi Leblond and Simon Lacoste-Julien

{
% \hspace{1.8em}\includegraphics[width=0.2\linewidth]{img/fabian}
\hspace{4.8em}\includegraphics[width=0.2\linewidth]{img/remi}
\hspace{4.8em}\includegraphics[width=0.2\linewidth]{img/SLJ}
}\\
{\normalsize\vphantom{}
% \hspace{0.5em} Fabian Pedregosa
\hspace{4em} R\'emi Leblond \hspace{3em} Simon Lacoste--Julien}\\
% \vphantom{}\\
% \vphantom{}\includegraphics[width=0.3\linewidth]{img/logo_inria}
% \includegraphics[width=0.12\linewidth]{img/ens.png}
% \hspace{0.5em}\includegraphics[width=0.3\linewidth]{img/montreal}
% \hspace{0.5em}\includegraphics[width=0.2\linewidth]{img/mila}

\end{frame}

{\usebackgroundtemplate{%
\begin{picture}(40,280)
  \includegraphics[height=1\paperheight]{img/sync_bg}
  \end{picture}
  }
\section{Synchronous algorithms}
}

\begin{frame}{Optimization for machine learning}
Large part of problems in machine learning can be framed as optimization problems of the form
$$\minimize_x f(x) \defas \displaystyle\frac{1}{n}\sum^{n}_{i=1} f_i(x)$$
\begin{columns}
\begin{column}{0.6\textwidth}  %%<--- here
{\bfseries Gradient descent} \parencite{cauchy1847methode}. Descend along steepest direction ($-\nabla f(x)$)
$$x^+ = x - \gamma \nabla f(x)$$

{\bfseries Stochastic gradient descent} (SGD) \parencite{robbins1951sgd}. Select a random index $i$ and descent along $~\nabla f_i(x)$:
$$x^+ = x - \gamma \nabla f_i(x)$$

\end{column}
\begin{column}{0.4\textwidth}  %%<--- here
\includegraphics[width=\linewidth]{img/gd}

\vspace{2em}
\includegraphics[width=\linewidth]{img/sgd}
\vspace*{-2.5em}\begin{flushright}{\footnotesize images source: Francis Bach}\end{flushright}
\end{column}
\end{columns}
\end{frame}




\begin{frame}{Parallel synchronous gradient descent}

Computation of gradient is distributed among $k$ workers

\begin{figure}
{\centering\includegraphics[width=0.5\linewidth]{img/mapreduce}}
\end{figure}

\begin{columns}
\begin{column}{0.7\textwidth}  %%<--- here
\begin{itemize}
\item Workers can be: different computers, CPUs or GPUs

\item Popular frameworks: Spark, Tensorflow, PyTorch, neHadoop.
\end{itemize}
\end{column}
\begin{column}{0.3\textwidth}  %%<--- here
\includegraphics[width=\linewidth]{img/gpu}
\end{column}
\end{columns}
\end{frame}


\begin{frame}{Parallel synchronous gradient descent}

% {\bfseries Idea:} Parallelize computation of $\nabla f(\xx)$ .

\begin{enumerate}
\item Choose $n_1, \ldots n_k$ that sum to $n$.
\item Distribute computation of $\nabla f(\xx)$ among $k$ nodes
\begin{align*}
\nabla f(\xx) &= \frac{1}{n} \sum_{i=1} \nabla f_i(\xx)\\ &= \frac{1}{k} \large(\underbrace{\frac{1}{n_1}\sum_{i=1}^{n_1} \nabla f_i(\xx)}_{\text{done by worker 1}} + \ldots + \underbrace{\frac{1}{n_1}\sum_{i=n_{k-1}}^{n_k} \nabla f_i(\xx)}_{\text{done by worker $k$}} \large)
\end{align*}
% {\centering\includegraphics[width=0.5\linewidth]{img/mapredu
\item Perform the gradient descent update by a master node
$$
\xx^+ = \xx - \gamma \nabla f(\xx)
$$
\end{enumerate}
\pause
\faCheck~Trivial parallelization, same analysis as gradient descent.

\faClose~Synchronization step every iteration (3.).
\end{frame}

\begin{frame}{Parallel synchronous SGD}
Can also be extended to stochastic gradient descent.
\begin{enumerate}
\item Select $k$ samples $i_{0}, \ldots, i_{k}$ uniformly at random.
\item Compute in parallel $\nabla f_{i_t}$ on worker $t$
\item Perform the (mini-batch) stochastic gradient descent update
$$
\xx^+ = \xx - \gamma \frac{1}{k}\sum_{t=1}^k \nabla f_{i_t}(\xx)
$$
\end{enumerate}
\pause
\faCheck~Trivial parallelization, same analysis as (mini-batch) stochastic gradient descent.

\faClose~Synchronization step every iteration (3.).
% This is the parallelization implemented in deep learning software: Tensorflow, PyTorch, etc.
\end{frame}

{\usebackgroundtemplate{%
\begin{picture}(20,320)
  \includegraphics[height=1\paperheight]{img/async_bg}
  \end{picture}
  }
\section{ \color{mDarkTeal} Asynchronous algorithms}
}

\begin{frame}{Asynchronous SGD}

\begin{columns}
\begin{column}{0.5\textwidth}  %%<--- here
Synchronization is the bottleneck. \\

\vspace{1em}{\Large\faLightbulbO~}What if we just ignore it?
\end{column}
\begin{column}{0.5\textwidth}  %%<--- here
\begin{center}
\includegraphics[width=\linewidth]{img/synchronization}
\end{center}
\end{column}
\end{columns}

\pause
Hogwild~\parencite{hogwild2011}: each core runs SGD in parallel, without synchronization, and updates the same vector of coefficients.

{\bfseries In theory}: convergence under very strong assumptions.

{\bfseries In practice}: just works.

\end{frame}

\begin{frame}{Hogwild in more detail}


Each core follows the same procedure
\begin{enumerate}
\item Read the information from shared memory $\hat{\xx}$.
\item Sample $i \in \{1, \ldots, n\}$ uniformly at random.
\item Compute partial gradient $\nabla f_i(\hat{\xx})$.
\item Write the SGD update to shared memory $\xx = \xx - \gamma \nabla f_i(\hat{\xx})$.
\end{enumerate}
\pause
\begin{figure}
\centering\includegraphics[width=0.6\linewidth]{img/async_updates}
\end{figure}
\end{frame}

\begin{frame}{Hogwild is fast}

Hogwild can be very fast. 
But its still SGD...

\begin{figure}
\includegraphics[width=0.8\linewidth]{img/figure_comparison_hogwild}
\end{figure}

\begin{itemize}
\item With constant step size, bounces around the optimum.

\item With decreasing step size, slow convergence.

\item There are better alternatives (Emilie mentioned some)
\end{itemize}
\end{frame}

% if you want some excitement ...
{\usebackgroundtemplate{%
\begin{picture}(40,260)
  \includegraphics[height=1\paperheight]{img/bungee}
  \end{picture}
  }
\section{Looking for excitement? ... analyze asynchronous methods!}
}

\begin{frame}{Analysis of asynchronous methods}

Simple things become counter-intuitive, e.g, how to {\bfseries name} the iterates?

\begin{figure}
\vspace{-0.5em}\includegraphics[width=0.8\linewidth]{img/sync_vs_async}
\vspace{-0.5em}\end{figure}

\vspace{1em}{\Large \faWarning}~Iterates will change depending on the speed of processors

\note{

Many of the things we are used to in classical optimization just break down in the presence of asynchrony. Suddenly, we need to start worrying for things that we used to give for granted. One these things is how to name things. It sounds silly but its true,  

}
\end{frame}



\begin{frame}{Naming scheme in Hogwild}

{\bfseries Simple, intuitive and wrong}

Each time a core has finished writing to shared memory, increment iteration counter.

$\iff$ $\hat{\xx}_t = (t+1)$-th succesfull update.

Value of $\hat{\xx}_t$ and $i_t$ are not determined until the iteration has finished.

$\implies$ $\hat{\xx}_t$ and $i_t$  are not necessarily independent.

\end{frame}

\begin{frame}{Unbiased gradient estimate}
SGD-like algorithms crucially rely on the unbiased property $\mathbb{E}_i[\nabla f_i(\xx)] = \nabla f(\xx)$.

\vspace{1em}
For synchronous algorithms, follows from the uniform sampling of $i$ 
\begin{align*}
\mathbb{E}_i[\nabla f_i(\xx)] &= \sum_{i=1}^n \text{Proba}(\text{selecting}~i) \nabla f_i(\xx)\\
 &\stackrel{\text{uniform sampling}}{=} \sum_{i=1}^n \frac{1}{n} \nabla f_i(\xx) = \nabla f(\xx)
\end{align*}
\end{frame}

\begin{frame}{A problematic example}

Selecting $i$ uniformly at random is not enough. 

\pause
{\bfseries Illustration}: problem with two samples and two cores $f = \frac{1}{2}(f_1 + f_2)$. Computing $\nabla f_1$ is much expensive than $\nabla f_2$.

\pause 
Start at $\xx_0$. Because of the random sampling there are 4 possible scenarios:
\begin{enumerate}
\item Core 1 selects $f_1$, Core 2 selects $f_1$ $\implies$ $\xx_1 = \xx_0 - \nabla f_1(\xx)$
\item Core 1 selects $f_1$, Core 2 selects $f_2$ $\implies$ $\xx_1 = \xx_0 - \nabla f_2(\xx)$
\item Core 1 selects $f_2$, Core 2 selects $f_1$ $\implies$ $\xx_1 = \xx_0 - \nabla f_2(\xx)$
\item Core 1 selects $f_2$, Core 2 selects $f_2$ $\implies$ $\xx_1 = \xx_0 - \nabla f_2(\xx)$
\end{enumerate}
So we have
\begin{align*}
\mathbb{E}_i\left[\nabla f_i\right] &= \frac{1}{4}f_1 + \frac{3}{4} f_2 \\
&\neq \frac{1}{2}f_1 + \frac{1}{2} f_2 ~!!
\end{align*}
\end{frame}


{\usebackgroundtemplate{%
\begin{picture}(40,260)
  \includegraphics[height=1\paperheight]{img/dali}
  \end{picture}
  }
\section{~The Art of {Naming \color{mDarkTeal} Things}}
}

% So that doesn't help much

\begin{frame}{A new labeling scheme}

{\Large\faLightbulbO}~New way to name iterates.

\pause
\vspace{1em}
``After read'' labeling~\parencite{leblond2016Asaga}. Increment counter each time we \emph{read} the vector of coefficients from shared memory.

\pause
\vspace{1em}
{\Large \faCheck}~No dependency between $i_t$ and the cost of computing $\nabla f_{i_t}$.

\vspace{1em}
{\Large \faCheck}~Full analysis of Hogwild and other asynchronous methods in \emph{``Improved parallel stochastic optimization analysis for
incremental methods'', Leblond, P., and Lacoste-Julien} (submitted).
\end{frame}

\begin{frame}{Analysis}
Analysis inspired by the perturbed iterate framework of \parencite{mania2015perturbed}. Distinguishes two quantities:
\begin{itemize}
\item $\hat{\xx}_t =$ $(t+1)$-th fully completed read from shared memory. 
\item $\xx_{t+1} \defas \xx_{t} - \gamma g(\hat{\xx}_t)$ ``virtual'' iterate, its a definition and does not need to exist in memory (for SGD $g = \nabla f_i$).
\end{itemize}

Analysis is carried out in $\xx_{t+1}$. Finally,
relate $\xx_t$ to $\hat{\xx}_t$.
\end{frame}

\section{Asynchronous SAGA}


\begin{frame}{The SAGA algorithm}

{\bfseries Setting:}
$$
\minimize_x \frac{1}{n} \sum_{i=1}^n f_i(\xx)
$$ 

The {\bfseries SAGA} algorithm~\parencite{defazio2014saga}.

Select $i \in \{1, \ldots, n\}$ and compute $(\xx^+, \balpha^+)$ as
$$
\xx^+ = \xx - \gamma (\nabla f_i(\xx) - \balpha_i + \overline{\balpha})\,;~\balpha_i^+ = \nabla f_i(\xx)
$$
\begin{itemize}
\item Like SGD, update is unbiased, i.e., $\mathbb{E}_i[\nabla f_i(\xx) - \balpha_i + \overline{\balpha})] = \nabla f(\xx)$.

\item Unlike SGD, because of memory terms $\balpha$, variance $\rightarrow$ 0.
\end{itemize}
\vspace{1em}\pause

\metroset{block=fill}
\begin{alertblock}{Super easy to use in scikit-learn}
\includegraphics[width=0.8\linewidth]{img/code}
% \begin{verbatim}
% from sklearn.linear_model import LogisticRegression
% clf = LogisticRegression(solver='saga')
% \end{verbatim}
\end{alertblock}
\end{frame}


\begin{frame}{Sparse SAGA}

{\bfseries Need for a sparse variant of SAGA}
\begin{itemize}
\item A large part of large scale datasets are sparse.
\item For sparse datasets and generalized linear models (e.g., least squares, logistic regression, etc.), partial gradients $\nabla f_i$ are sparse too.
\item Asynchronous algorithms work best when updates are sparse.
\end{itemize}

\vspace{1em}
SAGA update is inefficient
$$
\xx^+ = \xx - \gamma (\underbrace{\nabla f_i(\xx)}_{\text{sparse}} - \underbrace{\balpha_i}_{\text{sparse}} + \underbrace{\overline{\balpha}}_{\color{orange}\text{dense!}})\,;~\balpha_i^+ = \nabla f_i(\xx)
$$
[many tricks are used in in scikit-learn that we cannot use in asynchronous version]

\end{frame}



\begin{frame}{Sparse SAGA}
Sparse variant of SAGA. Relies on

\begin{itemize}
\item Diagonal matrix $\boldsymbol{P}_i = $ projection onto the support of $\nabla f_i$
\item Diagonal matrix $D$ defined as $D_{j, j} = 1 / \text{number of times $\nabla_j f_i$ is nonzero}$.
\end{itemize}

\metroset{block=fill}
\begin{alertblock}{Sparse SAGA algorithm~\parencite{leblond2016Asaga}}
$$
\xx^+ = \xx - \gamma (\nabla f_i(\xx) - \balpha_i + \boldsymbol{P}_i \boldsymbol{D}\overline{\balpha})\,;~\balpha_i^+ = \nabla f_i(\xx)
$$
\end{alertblock}

\begin{itemize}
\item All operations are sparse, cost per iteration is $\mathcal{O}(\text{nonzeros in $\nabla f_i$})$.
\item Same convergence properties than SAGA, but with cheaper iterations in presence of sparsity.
\item Crucial insight: $\mathbb{E}_i[\boldsymbol{P}_i \boldsymbol{D}] = \boldsymbol{I}$.
\end{itemize}

\end{frame}


\begin{frame}{Asynchronous SAGA (ASAGA)}

\begin{itemize}
\item Each core runs an instance of Sparse SAGA.

\item Updates the same vector of coefficients $\balpha, \overline{\balpha}$.
\end{itemize}


{\bfseries Theory:} Under standard assumptions (bounded dalays), same convergence than SAGA
% \begin{algorithm}[H]
%   \begin{algorithmic}[1]
%   \STATE Initialize shared variables $x$ and $(\alpha_i)_{i=1}^n$
%   % \LOOP
% %      \STATE $\hat x = $ inconsistent read of $x$
% %    \STATE $\forall j$, $\hat \alpha_j = $ inconsistent read of $\alpha_j$
% %      \STATE \textcolor{blue}{Sample $i$}  uniformly in $\{1,...,n\}$
% %      \STATE Let $S_i$ be $f_i$'s support
% %      \STATE $[\bar \alpha]_{S_i} = \nicefrac{1}{n} \sum_{k=1}^n [\hat \alpha_k]_{S_i}$
% %      \STATE
% %      \STATE $[\delta x]_{S_i} = -\stepsize (f'_i(\hat x) - \hat \alpha_i + D_{i} [\bar \alpha]_{S_i})$
% %          \FOR{$v$ {\bfseries in} $S_i$}
% %           \STATE $[x]_v \leftarrow [x]_v + [\delta x]_v$      \hfill // atomic
% %         \STATE $[\alpha_i]_v \leftarrow [f'_i( \hat x )]_v$
% %         \STATE  // {\small `$\gets$' denotes a shared memory update.}
% %        \ENDFOR
%   % \ENDLOOP
%  \end{algorithmic}
%  \end{algorithm}

\end{frame}

\begin{frame}{Experiments}
\includegraphics[width=\linewidth]{img/results_asaga}
% \includegraphics[width=0.5\linewidth]{img/asaga_speedup}

\begin{itemize}
\end{itemize}

\end{frame}


\section{Extension to non-smooth optimization}



\begin{frame}{Composite objective}

Previous methods assume objective function is smooth.

Cannot be applied to Lasso, Group Lasso, box constraints, etc.


\vspace{5mm}
{\bfseries Objective}: minimize composite objective function:
$$
\minimize_{\xx} f(\xx) + h(\xx)~,~ \text{ with $f(x) = \textstyle \frac{1}{n}\sum_{i=1}^n f_i(\xx)$}
$$
where $f_i$ is smooth and $h$ is a block-separable (i.e., $h(\boldsymbol{x}) = \sum_{B} h([\boldsymbol{x}]_B)$) convex function for which we have access to its proximal operator.


XXX The SAGA algorithm of Defazio
\end{frame}



\begin{frame}{Sparse Proximal SAGA}

{\bfseries Sparse Proximal SAGA.}~\parencite{pedregosa2017proxasaga} Extension of Sparse SAGA to composite optimization problems
% \vspace{0.5em}\begin{equation*}\tag{SAGA}
% \xx^+ = \prox_{\gamma h}(\xx - \gamma (\nabla f_i(\xx) - \balpha_i + \overline{\balpha}))\,;~\balpha_i^+ = \nabla f_i(\xx)~.
% \end{equation*}
% 
% \begin{align}
%     \vphantom{\sum}\vv_i &= \nabla f_i(\xx) - \balpha_i + \tikzmarkin<2>{a}\DD_i\tikzmarkend{a} \overline{\balpha}\,\\
%     \vphantom{\sum}\xx^+ &= \prox_{\gamma \varphi_i}\big(\xx - \gamma \vv_i \big)\,;~
%     \balpha_i^+ = \nabla f_i(\xx)
% \end{align}

\onslide<2->{Like SAGA, it relies on \rnode{a}{unbiased gradient} estimate} \onslide<3->{ and \rnode{b}{proximal step}}

\[
\onslide<2->{
      \vv_i \rnode[t]{ae}{=} \nabla f_i(\xx) - \balpha_i + \boldsymbol{D}_i \overline{\balpha}\,; }
\onslide<3->{%
  ~\xx^+ = \rnode[t]{be}{\prox}_{\gamma \varphi_i}(\xx - \gamma \vv_i)\,;~\balpha_i^+ = \nabla f_i(\xx)}
\] 

 \onslide<2->{\nccurve[angleA=-90,angleB=90, linecolor=gray]{->}{a}{ae}}
 \onslide<3->{\nccurve[angleA=-90, angleB=90, linecolor=gray]{->}{b}{be}}
 
 \onslide<4->{
 {Unlike SAGA}, and $\varphi_i$ are designed to give sparse updates while verifying unbiasedness conditions.}
 
\onslide<5->{ {\bfseries Convergence:} same linear convergence rate as SAGA, with cheaper updates in presence of sparsity. 
 }
\end{frame}


\begin{frame}{Proximal Asynchronous SAGA (ProxASAGA)}

Each core
 runs Sparse Proximal SAGA asynchronously without locks and updates $\xx$, $\balpha$ and $\overline{\balpha}$ in shared memory.

\vspace{0.5em} \faRandom~All read/write operations to shared memory are \emph{inconsistent}, i.e., no performance destroying vector-level locks while reading/writing.

{\bfseries Convergence:} under sparsity assumptions, ProxASAGA converges with the same rate as the sequential algorithm $\implies$ theoretical linear speedup with respect to the number of cores.
\end{frame}


\begin{frame}{Empirical results}



% \begin{columns}[T] % align columns
% \begin{column}{.35\textwidth}
% \end{column}%
% \hfill%
% \begin{column}{.65\textwidth}
% \includegraphics[width=1\linewidth]{img/prox_asaga_1}
% \end{column}%
% \end{columns}

ProxASAGA vs competing methods on 3 large-scale datasets, $\ell_1$-regularized logistic regression

\vspace{0em}\begingroup
\fontsize{9pt}{10pt}\selectfont
\begin{table}
\centering
%\resizebox{\linewidth}{!}{
\begin{tabular}{lrrrrr}
\toprule
{\bfseries\sffamily Dataset} & \multicolumn{1}{c}{$n$} & \multicolumn{1}{c}{$p$} & {{density}} & \multicolumn{1}{c}{$L$} & $\Delta$\\
\midrule
{\bfseries\sffamily KDD 2010} & \hfill 19,264,097 & \hfill 1,163,024 & \hfill $10^{-6}$ & \hfill 28.12 & 0.15\\
{\bfseries\sffamily KDD 2012} & \hfill 149,639,105 & \hfill 54,686,452 & \hfill $2 \times 10^{-7}$ & \hfill $1.25$ & 0.85\\
{\bfseries\sffamily Criteo} & \hfill 45,840,617 & \hfill 1,000,000 & \hfill $4 \times 10^{-5}$ & \hfill $1.25$ & 0.89\\
\bottomrule
\end{tabular}
%}
%\vspace{-5mm}
\end{table}
\endgroup


\includegraphics[width=1\linewidth]{img/prox_asaga_1}

\end{frame}


\begin{frame}{Empirical results - Speedup}

$$
\text{Speedup} = \frac{\text{Time to $10^{-10}$ suboptimality 
on one core}}{\text{Time to same suboptimality on $k$ cores}}
$$

\includegraphics[width=\linewidth]{img/prox_asaga_2}

\begin{itemize}
\pause \item ProxASAGA achieves speedups between 6x and 12x on a 20 cores architecture.

\pause \vspace{0.5em}\item As predicted by theory, there is a high correlation between degree of sparsity and speedup.
\end{itemize}

\pause Thanks for your attention, see you at {\bfseries poster \#159}.
\end{frame}

\begin{frame}{XX}

\faGithub~Code is in github: \url{https://github.com/fabianp/ProxASAGA}

A very efficient implementation of SAGA can be found in scikit-learn
\end{frame}





\begin{frame}[noframenumbering, allowframebreaks]
\nocite{pedregosa2017proxasaga}
  % \frametitle{References}
  \printbibliography
 \end{frame}


\appendix


\begin{frame}{Supervised Machine Learning}

{\bfseries Data}: $n$ observations $(\boldsymbol{a}_i, b_i) \in \mathbb{R}^p \times \mathbb{R}$

{\bfseries Prediction function}: $h(\boldsymbol{a}, \boldsymbol{x}) \in \RR$


{\bfseries Motivating examples:}
\begin{itemize}
\item Linear prediction: $h(\boldsymbol{a}, \xx) = \xx^T\boldsymbol{a}$
\item Neural networks: $h(\boldsymbol{a}, \xx) = \xx_m^T \sigma(\xx_{m-1}\sigma(\cdots\xx_2^T\sigma(\xx_1^T\boldsymbol{a}))$
\end{itemize}

\only<1>{
\begin{tikzpicture}[
ampersand replacement=\&,
scale=0.6,
every node/.style={scale=0.6},
plain/.style={
  draw=none,
  fill=none,
  },
net/.style={
  matrix of nodes,
  nodes={
    draw,
    circle,
    inner sep=10pt
    },
  nodes in empty cells,
  column sep=2cm,
  row sep=-9pt
  },
>=latex
]
\matrix[net] (mat)
{
|[plain]| \parbox{1.3cm}{\centering \Large Input\\layer} \& |[plain]| \parbox{1.0cm}{\centering\Large Hidden\\layer} \& |[plain]| \parbox{1.3cm}{\centering \Large Output\\layer} \\
\& |[plain]| \\
|[plain]| \& \\
\& |[plain]| \\
  |[plain]| \& |[plain]| \\
\& \& \\
  |[plain]| \& |[plain]| \\
\& |[plain]| \\
  |[plain]| \& \\
\& |[plain]| \\    };
\foreach \ai [count=\mi ]in {2,4,...,10}
  \draw[<-] (mat-\ai-1) -- node[above] {$\boldsymbol{a}_{\mi}$} +(-2cm,0);
\foreach \ai in {2,4,...,10}
{\foreach \aii in {3,6,9}
  \draw[->] (mat-\ai-1) -- (mat-\aii-2);
}
\foreach \ai in {3,6,9}
  \draw[->] (mat-\ai-2) -- (mat-6-3);
\draw[->] (mat-6-3) -- node[above] {Ouput} +(2cm,0);
\end{tikzpicture}
}
\only<2>{
{\bfseries Minimize some distance (e.g., quadratic) between the prediction}
$$
\minimize_{\xx} \frac{1}{n}\sum_{i=1}^n \ell(\boldsymbol{b}_i, h(\boldsymbol{a}_i, \xx)) \stackrel{\text{notation}}{=} \frac{1}{n}\sum_{i=1}^n f_i(\xx)
$$
where popular examples of $\ell$ are
\begin{itemize}
\item Squared loss, $ \ell(\boldsymbol{b}_i, h(\boldsymbol{a}_i, \xx)) \defas (\boldsymbol{b}_i - h(\boldsymbol{a}_i, \xx))^2$
\item Logistic (softmax), $ \ell(\boldsymbol{b}_i, h(\boldsymbol{a}_i, \xx)) \defas \log(1 + \exp(-\boldsymbol{b}_i h(\boldsymbol{a}_i, \xx)))$
\end{itemize}
}

\note{
The kind of parallelization will depend on the problem.
The kind of problem that we will be interested in
The problem: supervised learning.
}


\end{frame}

\begin{frame}{Sparse Proximal SAGA}
For step size $\gamma = \frac{1}{5L}$ and $f$ $\mu$-strongly convex ($\mu > 0$), Sparse Proximal SAGA converges geometrically in expectation. At iteration $t$ we have
\vspace{0.5em}$$
\mathbb{E} \|\xx_t - \xx^*\|^2 \leq (1-\textstyle\frac{1}{5} \min\{\frac{1}{n}, \frac{1}{\kappa}\})^t\, C_0~,
$$

\vspace{1em}with $C_0 = \|\xx_0 - \xx^*\|^2 + \textstyle\frac{1}{5 L^2} \textstyle\sum_{i=1}^n\|{\boldsymbol\alpha}_i^0 - \nabla f_i(\xx^*)\|^2$ and $
\kappa = \frac{L}{\mu}$ (condition number).

\vspace{1.5em} {\bfseries Implications}
\begin{itemize}\setlength\itemsep{0.5em}
\vspace{0.5em}\item Same convergence rate than SAGA with cheaper updates.
\begin{itemize}
\vspace{0.3em}\item {\normalsize In the ``big data regime'' ($n \geq \kappa$):  rate in $\mathcal{O}(1/n)$}.
\vspace{0.3em}\item {\normalsize In the ``ill-conditioned regime'' ($n \leq \kappa$):  rate in $\mathcal{O}(1/\kappa)$}.
\end{itemize}
\item Adaptivity to strong convexity, i.e., no need to know strong convexity parameter to obtain linear convergence.
% \item Same convergence rate than SAGA:
\end{itemize}
\vspace{1.3em}
\end{frame}

\end{document}
