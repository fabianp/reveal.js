\documentclass[10pt, notes]{beamer}

\usetheme[background=dark, numbering=fraction]{metropolis}
\usepackage{appendixnumberbeamer}
\usepackage{color, xcolor}
\usepackage{dsfont}
\definecolor{mydarkblue}{rgb}{0,0.08,0.45}


\usepackage{booktabs}
\usepackage[scale=2]{ccicons}


\usepackage{adjustbox} 
\usepackage{pgfplots}
\usepgfplotslibrary{dateplot}
\usepackage{fourier-orns}
\usepackage{xspace}



\usepackage[backend=bibtex,style=authoryear]{biblatex}
\addbibresource{index.bib}
\renewcommand*{\bibfont}{\scriptsize}



\newcommand{\themename}{\textbf{\textsc{metropolis}}\xspace}
% \DeclareMathOperator*{\argmin}{arg\,min}
% \DeclareMathOperator*{\minimize}{minimize}
% \DeclareMathOperator*{\maximize}{maximize}

\makeatletter
\renewcommand{\metropolis@colors@dark}{%
  \setbeamercolor{normal text}{%
    fg=black!2,
    bg=mDarkTeal
  }%
  \usebeamercolor[fg]{normal text}%
}
\renewcommand{\metropolis@colors@light}{%
  \setbeamercolor{normal text}{%
    fg=mDarkTeal,
    bg=black!2
  }%
  \usebeamercolor[fg]{normal text}%
}
\makeatother

\usepackage{tikz}
\usetikzlibrary{matrix,chains,positioning,decorations.pathreplacing,arrows}


\title{\huge Parallel Optimization and Machine Learning}
\author{
\large{\bfseries Fabian Pedregosa}}
% {\normalsize{ includes work with Remi Leblond and Simon Lacoste-Julien}}}
% \author{
% \hspace{4.8em}\includegraphics[width=0.2\linewidth]{img/remi}
% \hspace{4.8em}\includegraphics[width=0.2\linewidth]{img/SLJ}
% }\\
% {\normalsize\vphantom{}\hspace{0.5em} Fabian Pedregosa \hspace{3.5em} R\'emi Leblond \hspace{2.2em} Simon Lacoste--Julien}\\
% \vphantom{}\\
% \vphantom{}\includegraphics[width=0.3\linewidth]{img/logo_inria}
% \includegraphics[width=0.12\linewidth]{img/ens.png}
% \hspace{0.5em}\includegraphics[width=0.3\linewidth]{img/montreal}
% \hspace{0.5em}\includegraphics[width=0.2\linewidth]{img/mila}
% }
% 
\institute{

{\centerline{\hspace{2em}\includegraphics[width=0.3\linewidth]{img/Berkeley_wordmark_gold_no_uc}\hspace*{2em}
\includegraphics[width=0.3\linewidth]{img/eth_logo_kurz_neg}\hspace*{2em}
{\includegraphics[width=0.25\linewidth]{img/eu}}
}}
}
%\titlegraphic{\hfill\includegraphics[height=1.5cm]{figures/UCBerkeley_wordmark_blue.eps}}
\date{\vspace{1em}\today~{Huawei Paris Research Center}}

\input defs.tex



\begin{document}

\maketitle

\metroset{background=light} % change background theme according to manual
{
\usebackgroundtemplate{%
\begin{picture}(40,260)
  \includegraphics[height=0.9\paperheight]{img/white-rabbit}
  \end{picture}
  }
\begin{frame}{About me}
\begin{columns}
\begin{column}{0.4\textwidth}  %%<--- here
\end{column}
\begin{column}{0.6\textwidth}  %%<--- here
\begin{itemize}
\item Engineer (2010-2012), Inria Saclay.
\item PhD (2012-2015, Inria Saclay)
\item Postdoc (2015-2016), Dauphine--ENS--Inria Paris.
\item Postdoc (2017-present), UC Berkeley - ETH Zurich (Marie-Curie fellowship, European Commission)
\end{itemize}
\end{column}
\end{columns}

\note{I should start by presenting myself. I started my career as an engineer }

\end{frame}
}

\metroset{background=dark} % change background theme according to manual

\begin{frame}{Motivation}
\begin{columns}[T] % align columns
\begin{column}{.5\textwidth}
\centering{Computer add in 1993}
\includegraphics[width=1.05\linewidth]{img/old_ad}
\end{column}%
\hfill%
\begin{column}{.5\textwidth}
\centering{Computer add in 2006}
\includegraphics[width=\linewidth]{img/2006_ad}
\end{column}%
\end{columns}
What has changed?
\pause
2006 = no longer mentions to speed of processors
\end{frame}


\begin{frame}{Moore's law}
\begin{quote}
The complexity for minimum component costs 
has increased at a rate of roughly a factor of 
two per year. Certainly over the short term this 
rate can be expected to continue
\end{quote}
Gordon Moore (Intel), 1965

\hspace{1em}\begin{quote}
OK, maybe a factor of two every two 
years.
\end{quote}
Gordon Moore (Intel), 1975 [paraphrased]
\end{frame}


\begin{frame}{40 years of CPU trends}
\centering\includegraphics[width=0.8\linewidth]{img/moore_law}

\begin{itemize}[<+->]
\item Speed of CPUs has stagnated since 2005;
\item Multi-core architectures are here to stay.
\end{itemize}
\pause {\bfseries Parallel algorithms needed to take advantage of modern CPUs.}
\end{frame}


\begin{frame}{Outline}

{\bfseries Goal of the talk:} overview and state of the art in parallel optimization methods in machine learning.

\medskip

\begin{columns}[T] % align columns
\begin{column}{.5\textwidth}
{\centering \bfseries Synchronous methods}
\begin{itemize}
\item MapReduce (and variants: Hadoop, Spark)
\item ADMM
\item COCOA
\end{itemize}
\end{column}%
\hfill%
\begin{column}{.5\textwidth}
{\centering \bfseries Asynchronous methods}
\begin{itemize}
\item Asynchronous SGD
\item Variance-reduced methods
\end{itemize}
\end{column}%
\end{columns}
\vspace{1em}

Along the way, analysis of algorithms, our recent work.

\end{frame}

\begin{frame}{Outline}
Parallel algorithms can be divided  large categories: 
synchronous and asynchronous methods.

\begin{figure}
\includegraphics[width=\linewidth]{img/sync_vs_async}
{\small Image credits: \parencite{peng2016arock}}
\end{figure}
\end{frame}


\begin{frame}{Supervised Machine Learning}

{\bfseries Data}: $n$ observations $(\boldsymbol{a}_i, b_i) \in \mathbb{R}^p \times \mathbb{R}$

{\bfseries Prediction function}: $h(\boldsymbol{a}, \boldsymbol{x}) \in \RR$


{\bfseries Motivating examples:}
\begin{itemize}
\item Linear prediction: $h(\boldsymbol{a}, \xx) = \xx^T\boldsymbol{a}$
\item Neural networks: $h(\boldsymbol{a}, \xx) = \xx_m^T \sigma(\xx_{m-1}\cdots)$
\end{itemize}

\only<1>{
\begin{tikzpicture}[
ampersand replacement=\&,
scale=0.6,
every node/.style={scale=0.6},
plain/.style={
  draw=none,
  fill=none,
  },
net/.style={
  matrix of nodes,
  nodes={
    draw,
    circle,
    inner sep=10pt
    },
  nodes in empty cells,
  column sep=2cm,
  row sep=-9pt
  },
>=latex
]
\matrix[net] (mat)
{
|[plain]| \parbox{1.3cm}{\centering \Large Input\\layer} \& |[plain]| \parbox{1.0cm}{\centering\Large Hidden\\layer} \& |[plain]| \parbox{1.3cm}{\centering \Large Output\\layer} \\
\& |[plain]| \\
|[plain]| \& \\
\& |[plain]| \\
  |[plain]| \& |[plain]| \\
\& \& \\
  |[plain]| \& |[plain]| \\
\& |[plain]| \\
  |[plain]| \& \\
\& |[plain]| \\    };
\foreach \ai [count=\mi ]in {2,4,...,10}
  \draw[<-] (mat-\ai-1) -- node[above] {$\boldsymbol{a}_{\mi}$} +(-2cm,0);
\foreach \ai in {2,4,...,10}
{\foreach \aii in {3,6,9}
  \draw[->] (mat-\ai-1) -- (mat-\aii-2);
}
\foreach \ai in {3,6,9}
  \draw[->] (mat-\ai-2) -- (mat-6-3);
\draw[->] (mat-6-3) -- node[above] {Ouput} +(2cm,0);
\end{tikzpicture}
}
\only<2>{
{\bfseries Minimize some distance (e.g., quadratic) between the prediction}
$$
\minimize_{\xx} \frac{1}{n}\sum_{i=1}^n \ell(\boldsymbol{b}_i, h(\boldsymbol{a}_i, \xx)) \stackrel{\text{notation}}{=} \frac{1}{n}\sum_{i=1}^n f_i(\xx)
$$
}



\note{
The kind of parallelization will depend on the problem.
The kind of problem that we will be interested in
The problem: supervised learning.
}


\end{frame}


\begin{frame}{XXX}
\begin{center}
{ Minimize $F(x) = \frac{1}{n}\sum^{n}_{i=1} f_i(x)$}
\end{center}

\begin{columns}
\begin{column}{0.5\textwidth}  %%<--- here
Gradient descent [Cauchy, 1847]: descend along $-\nabla F'(x)$

Updates of the form $$x^+ = x - \gamma \nabla F'(x)$$
Stochastic gradient descent (SGD) [Robbins and Monro, 1951]: select a random index $i$ and descent along $~\nabla f_i(x)$,

Updates of the form $$x^+ = x - \gamma \nabla f_i(x)$$

\end{column}
\begin{column}{0.5\textwidth}  %%<--- here
\includegraphics[width=\linewidth]{img/gd}

\vspace{2em}
\includegraphics[width=\linewidth]{img/sgd}

\end{column}
\end{columns}







Images source: Francis Bach

\end{frame}



% 
% \begin{frame}[fragile]{Generalization}
% This approach can be naturally generalized to $K$ classes.
% \begin{align}
% \Pr(Y_i=1) &= \frac{e^{\boldsymbol\beta_1 \cdot \mathbf{X}_i}}{1 + \sum_{k=1}^{K-1} e^{\boldsymbol\beta_k \cdot \mathbf{X}_i}} \nonumber \\
% \Pr(Y_i=2) &= \frac{e^{\boldsymbol\beta_2 \cdot \mathbf{X}_i}}{1 + \sum_{k=1}^{K-1} e^{\boldsymbol\beta_k \cdot \mathbf{X}_i}} \\
% \cdots & \cdots \\
% \Pr(Y_i=K-1) &= \frac{e^{\boldsymbol\beta_{K-1} \cdot \mathbf{X}_i}}{1 + \sum_{k=1}^{K-1} e^{\boldsymbol\beta_k \cdot \mathbf{X}_i}}
% \end{align}
% \end{frame}
% 
% 
% \begin{frame}[fragile]{Application}
% Lets use this to predict the future!
% 
% \pause
% Use the data from the religion dataset to predict how religious beliefs will evolve after 2017
% 
% \includegraphics[width=0.5\linewidth]{figures/religion.png}
% 
% \pause
% Suggestion: use scikit-learn's LogisticRegression class with {multi\_class='multinomial'}. Use as features the year and cohort. The model can predict probabilities with method predict\_proba.
% 
% \end{frame}
% 
% 
% 
% \begin{frame}[fragile]{Application}
% 
% 
% To predict, use the same data but with year and cohort shifted accordingly.
% 
% \includegraphics[width=0.5\linewidth]{figures/logistic_future.png}
% 
% 
% 
% \end{frame}
% 
% 
% 
\begin{frame}[noframenumbering, allowframebreaks]
\nocite{pedregosa2017proxasaga}
  % \frametitle{References}
  \printbibliography
 \end{frame}

\end{document}
