\documentclass[10pt]{beamer}

\usetheme[background=dark, numbering=fraction]{metropolis}
\usepackage{appendixnumberbeamer}
\usepackage{color, xcolor}
\usepackage{dsfont}
\definecolor{mydarkblue}{rgb}{0,0.08,0.45}


\usepackage{booktabs}
\usepackage[scale=2]{ccicons}


\usepackage{adjustbox} 
\usepackage{pgfplots}
\usepgfplotslibrary{dateplot}
\usepackage{fourier-orns}
\usepackage{xspace}


\newcommand{\themename}{\textbf{\textsc{metropolis}}\xspace}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\minimize}{minimize}
\DeclareMathOperator*{\maximize}{maximize}

\makeatletter
\renewcommand{\metropolis@colors@dark}{%
  \setbeamercolor{normal text}{%
    fg=black!2,
    bg=mDarkTeal
  }%
  \usebeamercolor[fg]{normal text}%
}
\renewcommand{\metropolis@colors@light}{%
  \setbeamercolor{normal text}{%
    fg=mDarkTeal,
    bg=black!2
  }%
  \usebeamercolor[fg]{normal text}%
}
\makeatother

\title{\huge Parallel Optimization and Machine Learning}
\author{
\large{\bfseries Fabian Pedregosa}}
% {\normalsize{ includes work with Remi Leblond and Simon Lacoste-Julien}}}
% \author{
% \hspace{4.8em}\includegraphics[width=0.2\linewidth]{img/remi}
% \hspace{4.8em}\includegraphics[width=0.2\linewidth]{img/SLJ}
% }\\
% {\normalsize\vphantom{}\hspace{0.5em} Fabian Pedregosa \hspace{3.5em} R\'emi Leblond \hspace{2.2em} Simon Lacoste--Julien}\\
% \vphantom{}\\
% \vphantom{}\includegraphics[width=0.3\linewidth]{img/logo_inria}
% \includegraphics[width=0.12\linewidth]{img/ens.png}
% \hspace{0.5em}\includegraphics[width=0.3\linewidth]{img/montreal}
% \hspace{0.5em}\includegraphics[width=0.2\linewidth]{img/mila}
% }
% 
\institute{

{\centerline{\hspace{3em}\includegraphics[width=0.3\linewidth]{img/Berkeley_wordmark_gold_no_uc}\hspace*{2em}
\includegraphics[width=0.3\linewidth]{img/eth_logo_kurz_neg}\hspace*{2em}
{\includegraphics[width=0.25\linewidth]{img/eu}}
}}
}
%\titlegraphic{\hfill\includegraphics[height=1.5cm]{figures/UCBerkeley_wordmark_blue.eps}}
\date{\today~{Huawei Paris Research Center}}

\begin{document}

\maketitle

\metroset{background=light} % change background theme according to manual
{
\usebackgroundtemplate{%
\begin{picture}(40,260)
  \includegraphics[height=0.9\paperheight]{img/white-rabbit}
  \end{picture}
  }
\begin{frame}{About me}
\begin{columns}
\begin{column}{0.4\textwidth}  %%<--- here
\end{column}
\begin{column}{0.6\textwidth}  %%<--- here
\begin{itemize}
\item Engineer (2010-2012), Inria Saclay.
\item PhD (2012-2015, Inria Saclay)
\item Postdoc (2015-2016), Dauphine-ENS.
\item Postdoc (2017-present), UC Berkeley - ETH Zurich.
\end{itemize}
\end{column}
\end{columns}
\end{frame}
}

\metroset{background=dark} % change background theme according to manual

\begin{frame}{Motivation}
\begin{columns}[T] % align columns
\begin{column}{.5\textwidth}
\centering{Computer add in 1993}
\includegraphics[width=1.05\linewidth]{img/old_ad}
\end{column}%
\hfill%
\begin{column}{.5\textwidth}
\centering{Computer add in 2006}
\includegraphics[width=\linewidth]{img/2006_ad}
\end{column}%
\end{columns}
What has changed?
\pause
2006 = no longer mentions to speed of processors
\end{frame}


\begin{frame}{Moore's law}
\begin{quote}
The complexity for minimum component costs 
has increased at a rate of roughly a factor of 
two per year. Certainly over the short term this 
rate can be expected to continue
\end{quote}
Gordon Moore (Intel), 1965
\end{frame}


\begin{frame}{40 years of CPU trends}
\centering\includegraphics[width=0.8\linewidth]{img/moore_law}

\begin{itemize}[<+->]
\item Speed of CPUs has stagnated since 2005;
\item Multi-core architectures are here to stay.
\end{itemize}
\pause {\bfseries Parallel algorithms needed to take advantage of modern CPUs.}
\end{frame}


\begin{frame}{Outline}

{\bfseries Goal of the talk} state of the art in parallel optimization methods in machine learning

\begin{columns}[T] % align columns
\begin{column}{.5\textwidth}
ADMM
\end{column}%
\hfill%
\begin{column}{.5\textwidth}

\end{column}%
\end{columns}

\end{frame}

\begin{frame}{Outline}
Parallel algorithms can be divided into two 
synchronous and asynchronous optimization

\begin{figure}
\includegraphics[width=\linewidth]{img/sync_vs_async}
\caption{Image credits: Wotao}
\end{figure}
\end{frame}

% 
% \begin{frame}{Logistic regression}
%     Pillar of supervised learning. One of the most common methods
% 
%     Two motivations
% 	\begin{itemize}
% 		\item As a probabilistic model.
% 		\item Mathematical optimization.
% 	\end{itemize}
% \end{frame}
% 
% 
% 
% \section{Probabilistic model}
% 
% \begin{frame}[fragile]{Probabilistic view}
% \begin{center}
% {\bfseries Motivation}: classification problem with two classes.
% 
% \vspace{1em}Classes = ``-1'' and ``1'', which represent outcomes such as pass/fail, win/lose, alive/dead or healthy/sick, etc.
% 
% \vspace{1em}
% \danger\alert{Despite its name, logistic regression is a model for classification and not regression.}
% \end{center}
% \end{frame}
% 
% \begin{frame}[fragile]{Probabilistic view}
% \begin{center}
% {\bfseries Motivation}: Cancer / no cancer as a function of biomarker.
% \includegraphics[width=0.8\linewidth]{figures/logistic_1.pdf}
% 
% \end{center}
% \end{frame}
% 
% \begin{frame}[fragile]{Probabilistic view}
% \begin{center}
% {\bfseries Motivation}: Cancer / no cancer as a function of biomarker.
% \includegraphics[width=0.8\linewidth]{figures/logistic_2.pdf}
% 
% \pause
% \alert{Goal}: Given new data, estimate the probability of having cancer \hfill \\
% $\iff$ estimate {$P(Y=1|X)$}
% \end{center}
% \end{frame}
% 
% 
% \begin{frame}[fragile]{Probabilistic view}
% 
% 
% \begin{columns}[t]
% \begin{column}{.5\textwidth}
% One popular model for  {$P(Y=1|X)$} is the logistic model,
% 
% \begin{align*}
% &P(Y=y_i|X=x_i) = \sigma(y_i(x_i^T \beta_1 + \beta_0))\\
% &\quad \text{with } \sigma(t) = \frac{\exp(t)}{1 + \exp(t)}
% \end{align*}
% 
% \end{column}
% \begin{column}{.5\textwidth}
% \adjincludegraphics[width=0.8\linewidth,valign=t]{figures/sigmoid}\hfill\\
% The logistic function $\sigma(t)$
% \end{column}
% \end{columns}
% 
% \vspace{1em}
% In the 1D case we have 2 degrees of freedom, $\beta_1, \beta_0$ that control the slope and intercept of the approximation.
% \pause
% In the $p$-dimensional case, we have $p+1$-degrees of freedom, as $\beta_1 \in \mathbb{R}^p, \beta_i \in \mathbb{R}$
% \end{frame}
% 
% 
% \begin{frame}[fragile]{Inference}
% The coefficients $\beta_1, \beta_0$ can be estimated as the ones that maximize the likelihood given the current data $\{(y_1, x_1), \ldots, (x_n, y_n)\}$.
% $$
% \maximize_{\beta_1, \beta_0}\,\ell(\beta_1, \beta_0) = \prod^n_{i=1} P(Y=y_i|X=x_i)
% $$
% 
% \pause
% Although for numerical reasons we rather minimize the minus log-likelihood
% \begin{align}
% \minimize_{\beta_1, \beta_0}\,-\log(\ell(\beta_1, \beta_0)) &= \sum^n_{i=1} -\log(P(Y=y_i|X=x_i))\\
% &= \sum^n_{i=1} \log(1 + \exp(-y_i ( x_i^T \beta_1 + \beta_0)))\label{eq:log_loss}
% \end{align}
% \end{frame}
% 
% \section{Regularization}
% 
% \begin{frame}
% In the $p$-dimensional setting. 
% \end{frame}
% 
% \begin{frame}[fragile]{Generalization}
% This approach can be naturally generalized to $K$ classes.
% \begin{align}
% \Pr(Y_i=1) &= \frac{e^{\boldsymbol\beta_1 \cdot \mathbf{X}_i}}{1 + \sum_{k=1}^{K-1} e^{\boldsymbol\beta_k \cdot \mathbf{X}_i}} \nonumber \\
% \Pr(Y_i=2) &= \frac{e^{\boldsymbol\beta_2 \cdot \mathbf{X}_i}}{1 + \sum_{k=1}^{K-1} e^{\boldsymbol\beta_k \cdot \mathbf{X}_i}} \\
% \cdots & \cdots \\
% \Pr(Y_i=K-1) &= \frac{e^{\boldsymbol\beta_{K-1} \cdot \mathbf{X}_i}}{1 + \sum_{k=1}^{K-1} e^{\boldsymbol\beta_k \cdot \mathbf{X}_i}}
% \end{align}
% \end{frame}
% 
% 
% \begin{frame}[fragile]{Application}
% Lets use this to predict the future!
% 
% \pause
% Use the data from the religion dataset to predict how religious beliefs will evolve after 2017
% 
% \includegraphics[width=0.5\linewidth]{figures/religion.png}
% 
% \pause
% Suggestion: use scikit-learn's LogisticRegression class with {multi\_class='multinomial'}. Use as features the year and cohort. The model can predict probabilities with method predict\_proba.
% 
% \end{frame}
% 
% 
% 
% \begin{frame}[fragile]{Application}
% 
% 
% To predict, use the same data but with year and cohort shifted accordingly.
% 
% \includegraphics[width=0.5\linewidth]{figures/logistic_future.png}
% 
% 
% 
% \end{frame}
% 
% 
% 
\nocite{*}

\begin{frame}[allowframebreaks]{References}

  \bibliography{demo}
  \bibliographystyle{abbrv}

\end{frame}

\end{document}
